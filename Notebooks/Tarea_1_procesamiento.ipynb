{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e52c25",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "64f67e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f1fd9",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "976b7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_DATA = \"data/\"\n",
    "PATH_TO_RESULTS = \"results/\"\n",
    "\n",
    "docs_raw_path = f\"{PATH_TO_DATA}docs-raw-texts/\"\n",
    "queries_raw_path = f\"{PATH_TO_DATA}queries-raw-texts/\"\n",
    "bsii_results_file = f\"{PATH_TO_RESULTS}BSII-AND-queries_results.tsv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1460a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando caracteres no deseados y convirtiendo a minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a normalizar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto normalizado.\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Reemplaza múltiples espacios por uno solo\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Elimina referencias numéricas\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "4348f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en una lista de tokens utilizando expresiones regulares.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: La lista de tokens.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    pattern = r'''(?x)\n",
    "        (?:[A-Z]\\.)+[A-Z]?                      # abreviaturas: U.S.A, U.S.A.\n",
    "    | [A-Za-z]+(?:-[A-Za-z]+)*                  # palabras con guiones\n",
    "    | [A-Za-z]+(?:'[A-Za-z]+)?                  # contracciones: don't, we're\n",
    "    | \\$?\\d+(?:\\.\\d+)?%?                        # números simples\n",
    "    | \\.\\.\\.                                    # puntos suspensivos\n",
    "    | [\\[\\].,;\"'?():_`!-]                       # puntuación expandida\n",
    "    '''\n",
    "    tokens = nltk.regexp_tokenize(text, pattern)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "263cfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Elimina las stopwords de una lista de tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): La lista de tokens a procesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: La lista de tokens sin stopwords.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "49932170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Aplica stemming a una lista de tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): La lista de tokens a procesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: La lista de tokens con stemming aplicado.\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e954c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto aplicando normalización, tokenización, eliminación de stopwords y stemming.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: La lista de tokens preprocesados.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434853b",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "fe0ddd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_naf_data(dir_naf:str) -> dict:\n",
    "    \"\"\"\n",
    "    Carga los datos en formato NAF desde un directorio dado.\n",
    "    Para cada documento se concatena el título y el texto en bruto.\n",
    "    Luego de eso se procesa el texto, se tokeniza y se devuelve.\n",
    "\n",
    "    Args:\n",
    "        dir_naf (str): El directorio que contiene los archivos NAF.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con los IDs de los documentos como claves y listas de tokens como valores.\n",
    "    \"\"\"\n",
    "    dirp = Path(dir_naf)\n",
    "    out = {}\n",
    "    for naf_path in sorted(dirp.glob(\"*.naf\")):\n",
    "        root = ET.parse(naf_path).getroot()\n",
    "        public_id = root.find(\"./nafHeader/public\").attrib[\"publicId\"].strip()\n",
    "        title = root.find(\"./nafHeader/fileDesc\").attrib[\"title\"].strip()\n",
    "        raw_text = root.find(\"raw\").text\n",
    "        payload = raw_text[9:-3].strip()\n",
    "        combinado = f\"{title} {payload}\"\n",
    "        tokens = preprocess_text(combinado)\n",
    "        out[public_id] = tokens\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "54b4e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_naf_data(docs_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f223f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eugenio',\n",
       " 'beltrami',\n",
       " 'non-euclidian',\n",
       " 'geometri',\n",
       " 'eltrami',\n",
       " 'non-euclidian',\n",
       " 'geometri',\n",
       " '.',\n",
       " 'eugenio',\n",
       " 'beltrami',\n",
       " '(',\n",
       " '1835',\n",
       " '-',\n",
       " '1900',\n",
       " ')',\n",
       " '.',\n",
       " 'novemb',\n",
       " '16',\n",
       " ',',\n",
       " '1835',\n",
       " ',',\n",
       " 'italian',\n",
       " 'mathematician',\n",
       " 'eugenio',\n",
       " 'beltrami',\n",
       " 'born',\n",
       " '.',\n",
       " 'notabl',\n",
       " 'work',\n",
       " 'concern',\n",
       " 'differenti',\n",
       " 'geometri',\n",
       " 'mathemat',\n",
       " 'physic',\n",
       " '.',\n",
       " 'work',\n",
       " 'note',\n",
       " 'especi',\n",
       " 'clariti',\n",
       " 'exposit',\n",
       " '.',\n",
       " 'first',\n",
       " 'prove',\n",
       " 'consist',\n",
       " 'non-euclidean',\n",
       " 'geometri',\n",
       " 'model',\n",
       " 'surfac',\n",
       " 'constant',\n",
       " 'curvatur',\n",
       " ',',\n",
       " 'pseudospher',\n",
       " '.',\n",
       " 'eugenio',\n",
       " 'beltrami',\n",
       " 'born',\n",
       " 'cremona',\n",
       " 'lombardi',\n",
       " ',',\n",
       " 'part',\n",
       " 'austrian',\n",
       " 'empir',\n",
       " ',',\n",
       " 'part',\n",
       " 'itali',\n",
       " '.',\n",
       " 'son',\n",
       " 'artist',\n",
       " 'paint',\n",
       " 'miniatur',\n",
       " ',',\n",
       " 'young',\n",
       " 'eugenio',\n",
       " 'certain',\n",
       " 'inherit',\n",
       " 'artist',\n",
       " 'talent',\n",
       " 'famili',\n",
       " ',',\n",
       " 'case',\n",
       " 'addit',\n",
       " 'mathemat',\n",
       " 'talent',\n",
       " 'would',\n",
       " 'acquir',\n",
       " ',',\n",
       " 'musicrath',\n",
       " 'paint',\n",
       " 'becam',\n",
       " 'import',\n",
       " 'life',\n",
       " '.',\n",
       " 'began',\n",
       " 'studi',\n",
       " 'mathemat',\n",
       " 'univers',\n",
       " 'pavia',\n",
       " '1853',\n",
       " ',',\n",
       " 'expel',\n",
       " 'ghislieri',\n",
       " 'colleg',\n",
       " '1856',\n",
       " 'due',\n",
       " 'polit',\n",
       " 'opinion',\n",
       " '.',\n",
       " 'time',\n",
       " 'taught',\n",
       " 'influenc',\n",
       " 'francesco',\n",
       " 'brioschi',\n",
       " ',',\n",
       " 'appoint',\n",
       " 'professor',\n",
       " 'appli',\n",
       " 'mathemat',\n",
       " 'univers',\n",
       " 'pavia',\n",
       " 'year',\n",
       " 'beltrami',\n",
       " 'began',\n",
       " 'studi',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'discontinu',\n",
       " 'studi',\n",
       " 'financi',\n",
       " 'hardship',\n",
       " 'spent',\n",
       " 'next',\n",
       " 'sever',\n",
       " 'year',\n",
       " 'secretari',\n",
       " 'work',\n",
       " 'lombardi',\n",
       " 'venic',\n",
       " 'railroad',\n",
       " 'compani',\n",
       " 'first',\n",
       " 'verona',\n",
       " 'later',\n",
       " 'milan',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'milan',\n",
       " 'kingdom',\n",
       " 'itali',\n",
       " 'establish',\n",
       " '1861',\n",
       " ',',\n",
       " 'import',\n",
       " 'polit',\n",
       " 'event',\n",
       " 'much',\n",
       " 'invigor',\n",
       " 'academ',\n",
       " 'scene',\n",
       " 'itali',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'began',\n",
       " 'work',\n",
       " 'hard',\n",
       " 'mathemat',\n",
       " 'studi',\n",
       " '1862',\n",
       " 'publish',\n",
       " 'first',\n",
       " 'paper',\n",
       " '.',\n",
       " 'result',\n",
       " ',',\n",
       " 'appoint',\n",
       " 'univers',\n",
       " 'bologna',\n",
       " 'professor',\n",
       " '1862',\n",
       " '.',\n",
       " '1870',\n",
       " ',',\n",
       " 'new',\n",
       " 'univers',\n",
       " 'rome',\n",
       " 'set',\n",
       " 'new',\n",
       " 'italian',\n",
       " 'capit',\n",
       " 'beltrami',\n",
       " 'appoint',\n",
       " 'chair',\n",
       " 'ration',\n",
       " 'mechan',\n",
       " '1873',\n",
       " '.',\n",
       " 'three',\n",
       " 'year',\n",
       " 'rome',\n",
       " ',',\n",
       " 'beltrami',\n",
       " 'move',\n",
       " 'pavia',\n",
       " 'take',\n",
       " 'chair',\n",
       " 'mathemat',\n",
       " 'physic',\n",
       " '.',\n",
       " 'howev',\n",
       " ',',\n",
       " 'beltrami',\n",
       " 'return',\n",
       " 'rome',\n",
       " '1891',\n",
       " 'spent',\n",
       " 'last',\n",
       " 'year',\n",
       " 'teach',\n",
       " '.',\n",
       " 'becam',\n",
       " 'presid',\n",
       " 'accademia',\n",
       " 'dei',\n",
       " 'lincei',\n",
       " '1898',\n",
       " ',',\n",
       " 'follow',\n",
       " 'year',\n",
       " ',',\n",
       " 'senat',\n",
       " 'kingdom',\n",
       " '.',\n",
       " 'lover',\n",
       " 'music',\n",
       " ',',\n",
       " 'beltrami',\n",
       " 'interest',\n",
       " 'relationship',\n",
       " 'mathemat',\n",
       " 'music',\n",
       " '.',\n",
       " '.',\n",
       " 'c',\n",
       " '.',\n",
       " 'escher',\n",
       " ',',\n",
       " 'circl',\n",
       " 'limit',\n",
       " 'iv',\n",
       " ',',\n",
       " 'illustr',\n",
       " 'hyperbol',\n",
       " 'geometri',\n",
       " '1868',\n",
       " 'beltrami',\n",
       " 'publish',\n",
       " 'two',\n",
       " 'memoir',\n",
       " 'deal',\n",
       " 'consist',\n",
       " 'interpret',\n",
       " 'non-euclidean',\n",
       " 'geometri',\n",
       " 'bolyai',\n",
       " 'lobachevski',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'propos',\n",
       " 'geometri',\n",
       " 'could',\n",
       " 'realiz',\n",
       " 'surfac',\n",
       " 'constant',\n",
       " 'negat',\n",
       " 'curvatur',\n",
       " ',',\n",
       " 'pseudospher',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'concept',\n",
       " ',',\n",
       " 'line',\n",
       " 'geometri',\n",
       " 'repres',\n",
       " 'geodes',\n",
       " 'pseudospher',\n",
       " 'theorem',\n",
       " 'non-euclidean',\n",
       " 'geometri',\n",
       " 'prove',\n",
       " 'within',\n",
       " 'ordinari',\n",
       " 'three-dimension',\n",
       " 'euclidean',\n",
       " 'space',\n",
       " ',',\n",
       " 'deriv',\n",
       " 'axiomat',\n",
       " 'fashion',\n",
       " ',',\n",
       " 'lobachevski',\n",
       " 'bolyai',\n",
       " 'done',\n",
       " 'previous',\n",
       " '.',\n",
       " 'alreadi',\n",
       " '1840',\n",
       " ',',\n",
       " 'mind',\n",
       " 'alreadi',\n",
       " 'consid',\n",
       " 'geodes',\n",
       " 'triangl',\n",
       " 'pseudospher',\n",
       " 'remark',\n",
       " 'correspond',\n",
       " 'trigonometr',\n",
       " 'formula',\n",
       " 'obtain',\n",
       " 'correspond',\n",
       " 'formula',\n",
       " 'spheric',\n",
       " 'trigonometri',\n",
       " 'replac',\n",
       " 'usual',\n",
       " 'trigonometr',\n",
       " 'function',\n",
       " 'hyperbol',\n",
       " 'function',\n",
       " 'second',\n",
       " 'memoir',\n",
       " 'fundament',\n",
       " 'theori',\n",
       " 'space',\n",
       " 'constant',\n",
       " 'curvatur',\n",
       " ',',\n",
       " 'beltrami',\n",
       " 'continu',\n",
       " 'logic',\n",
       " 'gave',\n",
       " 'abstract',\n",
       " 'proof',\n",
       " 'equiconsist',\n",
       " 'hyperbol',\n",
       " 'euclidean',\n",
       " 'geometri',\n",
       " 'dimens',\n",
       " '.',\n",
       " 'accomplish',\n",
       " 'introduc',\n",
       " 'sever',\n",
       " 'model',\n",
       " 'non-euclidean',\n",
       " 'geometri',\n",
       " 'known',\n",
       " 'beltrami',\n",
       " 'klein',\n",
       " 'model',\n",
       " ',',\n",
       " 'poincar',\n",
       " 'disk',\n",
       " 'model',\n",
       " ',',\n",
       " 'poincar',\n",
       " 'half-plan',\n",
       " 'model',\n",
       " ',',\n",
       " 'togeth',\n",
       " 'transform',\n",
       " 'relat',\n",
       " '.',\n",
       " 'although',\n",
       " 'today',\n",
       " 'beltrami',\n",
       " 'essay',\n",
       " 'recogn',\n",
       " 'import',\n",
       " 'develop',\n",
       " 'non-euclidean',\n",
       " 'geometri',\n",
       " ',',\n",
       " 'recept',\n",
       " 'time',\n",
       " 'less',\n",
       " 'enthusiast',\n",
       " '.',\n",
       " 'beltrami',\n",
       " 'also',\n",
       " 'work',\n",
       " 'optic',\n",
       " ',',\n",
       " 'thermodynam',\n",
       " ',',\n",
       " 'elast',\n",
       " ',',\n",
       " 'electr',\n",
       " 'magnet',\n",
       " '.',\n",
       " 'contribut',\n",
       " 'topic',\n",
       " 'appear',\n",
       " 'four-volum',\n",
       " 'work',\n",
       " ',',\n",
       " 'oper',\n",
       " 'matematich',\n",
       " '(',\n",
       " '1902',\n",
       " '-',\n",
       " '20',\n",
       " ')',\n",
       " ',',\n",
       " 'publish',\n",
       " 'posthum',\n",
       " '.',\n",
       " 'yovisto',\n",
       " ',',\n",
       " 'learn',\n",
       " 'non-euclidian',\n",
       " 'geometri',\n",
       " 'histori',\n",
       " 'mathemat',\n",
       " 'lectur',\n",
       " 'professor',\n",
       " 'n',\n",
       " '.',\n",
       " 'j',\n",
       " '.',\n",
       " 'wildberg',\n",
       " 'mathhist',\n",
       " '12',\n",
       " 'non-euclidian',\n",
       " 'geometr']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.get(\"d006\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2f116",
   "metadata": {},
   "source": [
    "## Índice invertido (Inverted Index) (BSII) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9e9c911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(documents: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Construye un índice invertido a partir de los documentos tokenizados.\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Un diccionario con los IDs de los documentos como claves y listas de tokens como valores.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un índice invertido donde cada término tiene 'df' (document frequency) y 'postings' (lista ordenada de doc_ids).\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "    \n",
    "    for doc_id, tokens in documents.items():\n",
    "        unique_tokens = set(tokens)\n",
    "        for token in unique_tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(doc_id)\n",
    "    \n",
    "    # Convertir a formato con df y postings ordenados\n",
    "    for token in inverted_index:\n",
    "        postings = sorted(inverted_index[token])  # Ordenar posting list\n",
    "        inverted_index[token] = {\n",
    "            'docfreq': len(postings),                  # Document frequency\n",
    "            'postings': postings                  # Posting list ordenada\n",
    "        }\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "6e813fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = build_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b3ba136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docfreq': 5, 'postings': ['d009', 'd189', 'd194', 'd283', 'd325']}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.get(\"exam\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "87713ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docfreq': 2, 'postings': ['d006', 'd262']}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.get(\"beltrami\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02174a12",
   "metadata": {},
   "source": [
    "### Funciones de búsqueda booleana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0c698",
   "metadata": {},
   "source": [
    "Manning et al., Capítulo 1.3: \"Processing Boolean queries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9026c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and(list1: List[str], list2: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Mezcla dos listas ORDENADAS de resultados de búsqueda utilizando la operación AND.\n",
    "\n",
    "    Se emplean dos punteros para recorrer ambas listas de manera eficiente.\n",
    "\n",
    "    Args:\n",
    "        list1 (List[str]): La primera lista de resultados.\n",
    "        list2 (List[str]): La segunda lista de resultados.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Una lista ordenada de resultados que están en ambas listas.\n",
    "    \"\"\"\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(list1) and j < len(list2):\n",
    "        if list1[i] == list2[j]:\n",
    "            result.append(list1[i])  # Documento en ambas listas\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            i += 1  # Avanza en list1\n",
    "        else:\n",
    "            j += 1  # Avanza en list2\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a40986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_not(list1: List[str], list2: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Mezcla dos listas ORDENADAS de resultados de búsqueda utilizando la operación NOT.\n",
    "    Implementa una variación del algoritmo del recorrido con dos punteros.\n",
    "\n",
    "    Args:\n",
    "        list1 (List[str]): La primera lista de resultados.\n",
    "        list2 (List[str]): La segunda lista de resultados.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Una lista ordenada de los documentos que están en la primera lista pero no en la segunda.\n",
    "    \"\"\"\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    \n",
    "    while i < len(list1):\n",
    "        if j < len(list2) and list1[i] == list2[j]:\n",
    "            # Documento está en ambas - excluir\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif j < len(list2) and list1[i] > list2[j]:\n",
    "            # Avanzar j hasta alcanzar o pasar list1[i]\n",
    "            j += 1\n",
    "        else:\n",
    "            # list1[i] no está en list2 (o j se agotó) - incluir\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "588dce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(inverted_index: dict, query_terms: List[str], operation: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ejecuta consultas booleanas usando el algoritmo de mezcla.\n",
    "\n",
    "    La función soporta operaciones 'AND' y 'NOT'. Sin embargo, es importante tener en cuenta que:\n",
    "    - En una operación 'AND', si algún término no se encuentra en el índice invertido, el resultado será vacío.\n",
    "    - En una operación 'NOT', si el término base (el primer término) no se encuentra, el resultado será vacío.\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict): El índice invertido.\n",
    "        query_terms (List[str]): Los términos de búsqueda.\n",
    "        operation (str): La operación booleana ('AND' o 'NOT').\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Los documentos que coinciden con la consulta.\n",
    "    \"\"\"\n",
    "    if operation not in ['AND', 'NOT']:\n",
    "        raise ValueError(\"Operation must be 'AND' or 'NOT'\")\n",
    "    \n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    if operation == 'AND':\n",
    "        posting_lists = []\n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                posting_lists.append(inverted_index[term]['postings'])\n",
    "            else:\n",
    "                return []\n",
    "        \n",
    "        if not posting_lists:\n",
    "            return []\n",
    "        result = posting_lists[0]\n",
    "        for posting_list in posting_lists[1:]:\n",
    "            result = merge_and(result, posting_list)\n",
    "            if not result:\n",
    "                return []\n",
    "        return result\n",
    "    \n",
    "    elif operation == 'NOT':\n",
    "        \n",
    "        if query_terms[0] not in inverted_index:\n",
    "            return []\n",
    "        \n",
    "        result = inverted_index[query_terms[0]]['postings'].copy()\n",
    "        \n",
    "        for term in query_terms[1:]:\n",
    "            if term in inverted_index:\n",
    "                result = merge_not(result, inverted_index[term]['postings'])\n",
    "                if not result: \n",
    "                    return []\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f471edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_and_queries(inverted_index: dict, queries_folder: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Procesa todas las consultas AND desde archivos NAF y escribe resultados.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file))\n",
    "    with open(output_file, 'w') as f:\n",
    "\n",
    "        query_files = sorted([file for file in os.listdir(queries_folder) if file.endswith('.naf')])\n",
    "\n",
    "        for query_file in query_files:\n",
    "            # Parsear XML NAF\n",
    "            tree = ET.parse(os.path.join(queries_folder, query_file))\n",
    "            raw_content = tree.find('.//raw').text.strip()\n",
    "\n",
    "            public_id = tree.find('.//public').get('publicId') \n",
    "            \n",
    "            query_tokens = preprocess_text(raw_content)\n",
    "\n",
    "            results = boolean_search(inverted_index, query_tokens, 'AND')\n",
    "            if results:\n",
    "                f.write(f\"{public_id} {','.join(results)}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{public_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "219734bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docfreq': 35,\n",
       " 'postings': ['d016',\n",
       "  'd021',\n",
       "  'd024',\n",
       "  'd028',\n",
       "  'd032',\n",
       "  'd038',\n",
       "  'd060',\n",
       "  'd074',\n",
       "  'd082',\n",
       "  'd085',\n",
       "  'd094',\n",
       "  'd099',\n",
       "  'd100',\n",
       "  'd116',\n",
       "  'd123',\n",
       "  'd153',\n",
       "  'd170',\n",
       "  'd172',\n",
       "  'd179',\n",
       "  'd186',\n",
       "  'd212',\n",
       "  'd229',\n",
       "  'd234',\n",
       "  'd243',\n",
       "  'd254',\n",
       "  'd255',\n",
       "  'd265',\n",
       "  'd273',\n",
       "  'd275',\n",
       "  'd281',\n",
       "  'd284',\n",
       "  'd299',\n",
       "  'd315',\n",
       "  'd317',\n",
       "  'd329']}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.get(\"instrument\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a744a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_and_queries(inverted_index, queries_raw_path, bsii_results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
